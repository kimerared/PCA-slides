---
#title: "Understanding Principal Component Analysis (PCA)"
#author: "Hector Gavilanes, Gail Han, Michael Mezzano"
format:
  revealjs: 
    theme: serif
    transition: convex
    embed-resources: true
editor: visual
execute:
  echo: false
  warning: false
---

### 

![](images/01-Preface.jpg){fig-align="center"}

::: footer
Hector Gavilanes, Gail Han, Michael Mezzano
:::

# What is PCA?

-   Principal Component Analysis (PCA)
-   Dimensionality reduction technique.
-   Purpose:
    -   Simplification of complex datasets.
    -   Preservation of essential information.

# Why Use PCA?

-   Reducing Dimensionality: Simplify high-dimensional data.
-   Visualizing Data: Help visualize data in lower dimensions.
-   Noise Reduction: Eliminate less relevant features.
-   Improved Model Performance: Enhance machine learning efficiency.

# Methods

-   Data Matrix: Start with a matrix X of size n x m.
-   Centering: Subtract the mean of each feature.
-   Covariance Matrix: Calculate the covariance matrix of the centered data.
-   Select Principal Components: Choose top eigenvectors.

# Singular Value Decomposition (SVD)

-   Eigenvectors: Represent directions of maximum variance.
-   Eigenvalues: Indicate the variance explained by each eigenvector.
-   Sorting: Sort eigenvalues in descending order to select the most significant components.

# Variance Explained

-   Explained Variance Ratio: Calculate the ratio of each eigenvalue.
-   Cumulative Variance: Plot cumulative explained variance to determine components to retain.

# 

::: columns
::: {.column width="40%"}
## Objective

-   Weighted Combination

-   Maximal Variance Components
:::

::: {.column width="60%"}
### High Variance vs

### Low Variance

![](images/01-Preface.jpg){width="Infinity"}
:::
:::

# Dimensionality Reduction

-   Unsupervised Learning.
-   Reduce Dimensions: Transform data by multiplying with selected eigenvectors.
-   New Feature Space: Data exists in a lower-dimensional feature space.

# Visualization

-   Data Projection: Visualize data in the reduced feature space.
-   Scatterplots: Use scatterplots to visualize data distribution.

# Assumptions and Limitations

-   Interpretability: Loss of interpretability in transformed features.
-   Loss of Information: Reducing dimensionality may result in some information loss.
-   Scaling: Data scaling is important to avoid feature dominance.

# Applications of PCA

-   Image Compression: Reduce image size while preserving details.
-   Face Recognition: Reduce facial feature dimensions for classification.
-   Anomaly Detection: Identify anomalies in large datasets.
-   Bioinformatics: Analyze gene expression data.

# Dataset

::: panel-tabset
### Overview

-   39 variables, or features.

-   55 Observations.

### Description

-   Administered to In-Center Hemodialysis Survey.

-   State-level averages.

-   Dialysis Quality measures.

### Source

-   Released on July 19, 2023.

-   [data.cms.gov.](https://data.cms.gov/provider-data/dataset/2fpu-cgbb)
:::

::: footer
<div>

Data Obtained from "Consumer Assessment of

Healthcare Providers and Systems (CAHPS)."

</div>
:::

# Variables / Features

::: panel-tabset
### Index

-   "State," 50 states & 6 U.S. territories.
-   Categorical feature removed from model.

### Response

-   24 features: patient care quality ratings.
-   Transfusions, fistula usage, infections,
-   Hospitalizations, incident waitlisting, & readmissions.

### Dialysis

-   14 features: dialysis adequacy (Kt/V), type of dialysis,
-   Serum phosphorus level, & average hemoglobin level,
-   Normalized protein catabolic rate (nPCR), hypercalcemia level.
:::

# Dataset Selection Rationale

-   Driven by multicollinearity.

-   Features less significant in explaining variability.

-   All variables are numeric, except the index variable.

# Data Preparation

::: {.panel-tabset style="font-size: 85%;"}
### Editing

-   Efficient removal of white spaces in the dataset.

-   Editing variable names to enhance readability and meaningful.

### Example

`Original:` "Percentage.Of.Adult..Patients.With.Hypercalcemia..Serum.Calcium.Greater.Than.10.2.Mg.dL."

`Edited:` "hypercalcemia_calcium \> 10.2Mg."
:::

# PCA in Machine Learning

-   Feature Selection: Use PCA to select relevant features.
-   Model Training: Enhance model performance by reducing dimensionality.
-   Preprocessing: Standardize and normalize data before applying PCA.

# Orthogonality

```{r}
# Create orthogonal vectors
vector1 <- c(2, 3)
vector2 <- c(-3, 2)

# Plot the vectors
plot(c(0, vector1[1]), c(0, vector1[2]), type = "n", xlim = c(-5, 5), ylim = c(-5, 5), xlab = "X-axis", ylab = "Y-axis")
arrows(0, 0, vector1[1], vector1[2], length = 0.1, angle = 20, col = "blue")
arrows(0, 0, vector2[1], vector2[2], length = 0.1, angle = 20, col = "red")

# Add labels
text(vector1[1], vector1[2], "Vector 1", pos = 4, col = "blue")
text(vector2[1], vector2[2], "Vector 2", pos = 2, col = "red")

# Add a legend
legend("topright", legend = c("Vector 1", "Vector 2"), col = c("blue", "red"), lty = 1)

```

# Orthogonality V1

```{r}
library(ggplot2)
library(scatterplot3d)
library(MASS)  # For the mvrnorm function
library(rgl)
# Set the random number generator seed
set.seed(99)

# Generate multivariate normal distribution

X <- mvrnorm(n = 50, mu = c(0, 0, 0), Sigma = matrix(c(1, 0.2, 0.7, 0.2, 1, 0, 0.7, 0, 1), nrow = 3))

# Create a 3D scatter plot
scatterplot3d(X[, 1], X[, 2], X[, 3], color = "blue", pch = 16, grid = TRUE, main = "3D Scatter Plot")

# Set axis limits
maxlim <- max(abs(X)) * 1.1
scatterplot3d(X[, 1], X[, 2], X[, 3], color = "blue", pch = 16, grid = TRUE, main = "3D Scatter Plot",
              xlim = c(-maxlim, maxlim), ylim = c(-maxlim, maxlim), zlim = c(-maxlim, maxlim))

# Set equal aspect ratio
aspect3d(1)

# Set the view angle
#rgl.postscript('3d_plot.png', fmt = 'png', w = 800, h = 800)




```

# Conclusion

-   Summary: PCA is an unsupervised learning technique for dimensionality reduction and data visualization.
-   Key Takeaways: Understand eigenvectors, eigenvalues, and explained variance.

# Questions

-   Open the floor for questions from the audience.

# Thank You

-   Contact Information

# References

-   List of sources and references used in the presentation.
